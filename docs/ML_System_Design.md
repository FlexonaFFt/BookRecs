# ML System Design Doc
## Дизайн ML системы - Персональные рекомендации новинок в ленте BookRecs (Goodreads YA)

### 1. Цели и предпосылки 
#### 1.1. Зачем идем в разработку продукта?  

- **Бизнес-цель `Product Owner`**  
  Сейчас пользователю сложно быстро найти «свою» книгу из большого каталога → он тратит время или уходит без покупки. Мы хотим упростить этот выбор с помощью рекомендаций. **Что это дает бизнесу:**

  - увеличиваем выручку и маржинальность за счёт более релевантных предложений;
  - повышаем конверсию (пользователь быстрее находит, что купить);
  - помогаем продавать не только бестселлеры, но и новинки + возможно получится продать книги, которые давно лежат на складе, как "непопулярные";
  

- **Почему использование ML улучшит текущее решение?**
  Проекты, которые используют рекомендации без ML, обычно используют подход (популярное / новинки) и имеют ряд минусов: одинаковые рекомендации для всех пользователей, не учитываются личные предпочтения, не находятся менее очевидные, но релевантные книги.

  **Что может дать ML:**

  - персонализация — рекомендации под конкретного пользователя;
  - лучшее попадание в интересы, что увеличивает вероятность покупки;
  - возможность находить менее очевидные, но релевантные книги;


- **Что будем считать успехом итерации с точки зрения бизнеса `Product Owner`**  
  Успех оцениваем через A/B тест: сравниваем новую систему рекомендаций с текущей и смотрим, меняется ли поведение пользователей. В первую очередь ожидаем, что пользователи начнут чаще покупать — это должно отражаться в росте выручки на сессию и среднего чека. Дополнительно смотрим на более простые сигналы: кликают ли пользователи на рекомендации, чаще ли с ними взаимодействуют и быстрее ли находят подходящую книгу.


  При этом важно, чтобы мы не ухудшили базовый опыт: конверсия в покупку не должна падать, возвраты — расти, а система — становиться заметно медленнее. Если пользователи активнее взаимодействуют с рекомендациями, чаще покупают, и при этом ключевые метрики остаются стабильными, считаем решение успешным.

#### 1.2. Бизнес-требования и ограничения  

- **Краткое описание БТ**

  Мы хотим показывать пользователям персональные рекомендации:
  - предлагать книги, которые могут им понравиться (на основе их интересов и поведения);
  - показывать «похожие книги» на странице конкретной книги;
  За счет этого увеличивать продажи 
 
- **Бизнес-ограничения `Product Owner`**

  - Не стоит рекомендовать книги, которых нет в наличии;
  - Желательно объяснять пользователю, почему ему это показали (например: «похоже по жанру» или «из той же серии»); 
  
- **Что мы ожидаем от конкретной итерации `Product Owner`**

  - Делаем базовую версию (MVP) рекомендаций
  - Показываем их в нескольких местах: например, на главной, на странице книги и в корзине
  - Запускаем A/B тест на части пользователей
  - Фиксируем текущие метрики и сравниваем их с новой версией
  - Готовим возможность быстро откатиться, если что-то пойдёт не так

- **Как проходит пилот?**

  - В онлайне:
 
    - показываем блок с рекомендациями
    - фиксируем, что пользователь его увидел
    - отслеживаем, кликает ли он и покупает ли что-то

  - В офлайне:
 
    - если можем понять, кто пользователь, добавляем его покупки в историю
    - потом используем это для рекомендаций онлайн
    - если не можем идентифицировать — просто не учитываем офлайн в MVP
     
- **Пилот будет успешным, в случае, если:**

  Если пользователи начинают больше покупать. Например, растет выручка за сессию. И при этом: *не падает конверсия, не растут возвраты, проект работает стабильно без проблем.* 

#### 1.3. Что входит в скоуп проекта/итерации, что не входит   

- **Что закрываем в данной итерации (DS)**

  В этой итерации собираем рабочий MVP рекомендаций под задачу из `taskreadme.md`: персональные user2item рекомендации с устойчивостью к cold-start у книг.

  Что делаем в рамках MVP:
  - делаем два простых ориентира: случайные/популярные рекомендации и базовую модель по похожему поведению пользователей;
  - делаем основную модель, которая сочетает поведение пользователей и данные о книгах, чтобы рекомендовать и новые книги тоже;
  - считаем обязательные метрики: `NDCG@10`, `Recall@10`, `Coverage@10` и отдельные cold-метрики (`ColdNDCG@10`, `ColdRecall@10`);
  - делаем валидацию отдельно для warm и cold книг, потому что в тесте они несбалансированы;
  - сохраняем веса модели и артефакты инференса;
  - готовим выдачу топ-N рекомендаций для блока в интерфейсе и запасной вариант: если модель недоступна, показываем популярные или новые книги.

- **На закрытие каких БТ подписываемся в данной итерации `Data Scientist`**
  - MVP даёт персонализацию вместо одинаковой выдачи для всех;
  - новые книги не «тонут»: модель умеет рекомендовать cold-start айтемы;
  - есть честное сравнение с бейзлайном по метрикам и понятная таблица результатов;
  - есть материалы для запуска пилота A/B: базовый вариант, тестовый вариант, список целевых метрик и защитных метрик.

- **Что не будет закрыто `Data Scientist`**
  - не оптимизируем «до идеала» latency и стоимость (делаем разумный MVP уровень);
  - не делаем explainability «как в зрелом продукте» (только базовые причины показа);
  - не закрываем cold-start пользователей (в тесте соревнования только warm пользователи);
  - не делаем полноценный онлайн-контур переобучения в реальном времени.

- **Описание результата с точки зрения качества кода и воспроизводимости решения `Data Scientist`**
  - весь пайплайн воспроизводим: фиксируем `random seed`, версии библиотек и сплиты;
  - обучение, инференс и оценка разделены на отдельные шаги;
  - есть единая таблица метрик по всем подходам и отдельный срез по cold айтемам;
  - все ключевые гиперпараметры и конфиги вынесены в явный вид;
  - сохранение/загрузка моделей проверено, чтобы результаты можно было повторить без переобучения.
  
- **Соответствия требованиям соревнования GoodReeds** (отсюда взят датасет)
  - тестовый набор не изменяем;
  - делаем отдельную оценку для warm и cold книг;
  - показываем таблицу метрик для каждого подхода;
  - в таблице обязательно есть `NDCG@10`, `Recall@10`, `Coverage@10` и cold-метрика;
  - сохраняем веса обученных моделей;
  - итоговый MVP должен быть лучше случайного ориентира.

#### 1.4. Предпосылки решения  

- **Базовые предпосылки решения (DS)**
  - задача формулируется как персонализированный `top-N` ranking (а не прогноз рейтинга книги в абсолютных баллах);
  - целевой эффект для бизнеса — рост покупки через более релевантные рекомендации, поэтому оптимизируем ранжирование и покрытие каталога;
  - в данных много разреженности, а у части книг нет истории, поэтому чисто collaborative подход недостаточен — нужен гибрид.

- **Какие блоки данных используем**
  - логи взаимодействий: `impression -> click -> add_to_cart -> purchase` (implicit feedback);
  - справочник книг: жанры, авторы, серия, описание, год, язык и прочие метаданные;
  - служебные признаки для фильтров: доступность/наличие, язык, базовые бизнес-ограничения.

- **Горизонт и частота пересчёта**
  - прогноз как таковой не строим, считаем релевантность «на ближайший показ» (next best books в текущем сеансе/визите);
  - переобучение MVP: батчево (например, раз в день/несколько дней), без realtime online-learning;
  - фичи пользователя пересчитываются с упором на recent историю, чтобы не терять актуальные интересы.

- **Гранулярность модели**
  - сущности уровня `user_id x item_id` (книга);
  - выдача в формате `top-N` (обычно `N=10`) на пользователя/контекст показа;
  - оценка качества отдельно по всему каталогу и отдельно по cold-item сегменту.

- **Предпосылки по данным и экспериментам**
  - тестовый набор фиксированный и не изменяется;
  - в тесте есть cold и warm книги, но пользователи warm, поэтому делаем акцент на item cold-start;
  - метрики считаем на одинаковом протоколе для всех моделей, чтобы сравнение было честным;
  - успех MVP формулируем относительно бейзлайна, а не «в вакууме».

- **Ограничения MVP, которые принимаем осознанно**
  - если часть пользовательских идентификаторов теряется между каналами, офлайн сигналы используем ограниченно;
  - объяснимость рекомендаций в MVP простая (по похожести/жанру), без сложного explanation engine;
  - при деградации модели используем fallback на популярное/новинки, чтобы не ломать пользовательский опыт.

### 2. Методология `Data Scientist`     

#### 2.1. Постановка задачи  

- С технической стороны делаем **персональную рекомендательную систему книг** (задача ранжирования), а не прогноз продаж и не поиск аномалий.
- Для каждого пользователя формируем список `top-10` книг, которые с наибольшей вероятностью заинтересуют его сейчас.
- Учитываем два режима:
  - **warm-книги**: у книги уже есть история взаимодействий;
  - **cold-книги**: книга новая, истории почти нет или нет совсем.
- Основная цель модели: улучшить качество рекомендаций относительно простых ориентиров и не провалиться на cold-start книгах.
- Единица предсказания: пара `пользователь × книга`.
- Целевой сигнал: неявная обратная связь (просмотр, клик, добавление в корзину, покупка) с большим весом у более «сильных» действий.

#### 2.2. Блок-схема решения  

- Ниже две схемы: отдельно для базового решения и для основного MVP.

**Базовое решение (ориентир):**

```mermaid
flowchart LR
A["Логи действий пользователей + список книг"] 
    --> B["Очистка данных"]

B --> C{"Два подхода"}
C --> D["Популярные"]
C --> E["Схожесть"]

D --> F["Top-10"]
E --> F

F --> G["Метрики"]
```

**Основной MVP (целевая схема):**

- В этой схеме уже есть гибридная модель, фильтры и подготовка к пилоту.

```mermaid
flowchart LR
A["Логи действий + метаданные книг"] --> B["Подготовка признаков"]

B --> C1["Признаки пользователя\n(интересы, недавняя активность)"]
B --> C2["Признаки книги\n(контент и метаданные)"]

C1 --> D["Гибридная модель\n(поведение + контент)"]
C2 --> D

D --> E["Ранжированный список Top-10"]

E --> F["Бизнес-фильтры\n(например, доступность)"]

F --> G["Оценка качества\n(warm / cold сегменты)"]

G --> H["A/B пилот"]
G --> I["Fallback:\nпопулярные / новинки"]
```

#### 2.3. Этапы решения задачи `Data Scientist`  

- Ниже план работ по этапам. Для каждого этапа отдельно фиксируем, что делаем в базовом варианте и в MVP.

**Этап 1. Подготовка и проверка данных**

| Данные | Откуда берем | Нужны для | Проверки качества |
| --- | --- | --- | --- |
| История действий пользователя (просмотры/клики/покупки) | логи продукта | база + MVP | пропуски, дубли, корректность времени |
| Показ рекомендаций (если есть) | логи блока рекомендаций | MVP и A/B | полнота логов, корректность позиции |
| Справочник книг (жанр, автор, серия, описание) | каталог/метаданные | MVP (cold-start) | пустые поля, ошибки в категориях |
| Наличие книги | склад/каталог | база + MVP | актуальность статуса «в наличии» |

- Базовый вариант:
  - чистим данные;
  - делаем простую матрицу взаимодействий `пользователь × книга`.
- MVP:
  - дополнительно готовим признаки книги и пользователя;
  - проверяем, что холодные книги не выпали из витрины.
- Результат этапа:
  - единая обучающая выборка;
  - отдельные срезы warm/cold для оценки.
- Риск:
  - мало сигналов по новым книгам.
- Что делаем:
  - усиливаем метаданные и добавляем контентные признаки.

**Этап 2. Формирование выборок для обучения и проверки**

- Базовый вариант:
  - делим данные по времени (раньше -> обучение, позже -> проверка);
  - делаем негативные примеры из невыбранных книг.
- MVP:
  - сохраняем тот же принцип по времени;
  - отдельно контролируем долю cold-книг в проверке.
- Результат этапа:
  - прозрачные и повторяемые выборки без утечки из будущего.
- Риск:
  - случайное смешивание по времени завысит качество.
- Что делаем:
  - фиксируем временной разрез и запрещаем случайное перемешивание.

**Этап 3. Обучение базовых решений и MVP**

- Базовый вариант:
  - популярные/случайные рекомендации;
  - простая модель по похожему поведению пользователей.
- MVP:
  - гибридная модель: сигналы поведения + признаки книги;
  - настройка параметров на валидации.
- Результат этапа:
  - обученные модели и сохраненные веса.
- Риск:
  - модель может хорошо работать на warm и слабо на cold.
- Что делаем:
  - контролируем cold-метрики как обязательные, а не «дополнительные».

**Этап 4. Оценка качества и сравнение**

- Метрики:
  - `NDCG@10`, `Recall@10`, `Coverage@10`;
  - отдельно `ColdNDCG@10`, `ColdRecall@10`.
- Базовый вариант:
  - считаем метрики на общем наборе и фиксируем отправную точку.
- MVP:
  - считаем те же метрики + отдельно на cold сегменте;
  - сравниваем с базовым вариантом.
- Результат этапа:
  - единая таблица качества по всем подходам.
- Бизнес-проверка:
  - совместно с продуктом согласуем, что рост метрик достаточен для A/B пилота.

**Этап 5. Подготовка выдачи и бизнес-правил**

- Базовый вариант:
  - выдаем `top-10` без сложных доработок.
- MVP:
  - применяем фильтры: не показываем недоступные книги;
  - добавляем простой резервный сценарий (популярные/новинки), если модель недоступна.
- Результат этапа:
  - стабильная выдача для пилота без «пустых» ответов.
- Риск:
  - после фильтров список может стать слишком коротким.
- Что делаем:
  - добиваем список резервными кандидатами.

**Этап 6. Подготовка к пилоту**

- Базовый вариант:
  - фиксируем контрольный сценарий (текущая выдача/популярные).
- MVP:
  - фиксируем тестовый сценарий;
  - сохраняем версию модели и параметры, чтобы результаты можно было повторить.
- Результат этапа:
  - готовый пакет для запуска пилота: модели, метрики, описание рисков и условий отката.
  
### 3. Подготовка пилота  
  
#### 3.1. Способ оценки пилота  
  
- Пилот проводим как классический A/B тест на реальном трафике.
- Делим пользователей случайно на две группы:
  - **контроль**: текущая логика рекомендаций (или популярные книги, если текущей персонализации нет);
  - **тест**: новая модель рекомендаций из MVP.
- Пользователь всегда остается в своей группе на весь период пилота.
- Считаем эффект на уровне пользователя/сессии, а не по отдельным кликам, чтобы не завышать результат.
- Перед запуском фиксируем:
  - основную целевую метрику (рост покупок/выручки на сессию);
  - защитные метрики (конверсия в покупку, доля возвратов, задержка ответа, ошибки сервиса);
  - минимальную длительность теста и минимальный размер выборки.
- В ходе пилота каждый день проверяем корректность эксперимента:
  - равномерность разбиения групп;
  - целостность логов;
  - отсутствие критичных перекосов по сегментам.
- Итоговая оценка: сравниваем контроль и тест, смотрим статистическую значимость и практическую полезность эффекта.
  
#### 3.2. Что считаем успешным пилотом  
  
Пилот считаем успешным, если одновременно выполняются условия ниже:

- Основная метрика:
  - рост относительно контроля по **выручке на сессию** (фиксируем до старта пилота как единственную главную метрику).
- Дополнительная бизнес-метрика:
  - доля сессий с покупкой (смотрим как подтверждающий сигнал).
- Качество рекомендаций в продукте:
  - растет CTR блока рекомендаций;
  - не падает покрытие каталога (чтобы не показывать только одни и те же книги).
- Защитные метрики:
  - конверсия в покупку не ухудшается;
  - доля возвратов не растет;
  - задержка ответа сервиса рекомендаций остается в допустимом коридоре;
  - нет роста технических ошибок.
- Условия достоверности:
  - группы в A/B тесте сбалансированы;
  - длительности теста хватило, чтобы покрыть недельную сезонность;
  - эффект подтверждается статистически, а не выглядит как случайный шум.

Если основная метрика растет, а защитные метрики стабильны, принимаем решение о расширении пилота на больший трафик.
  
#### 3.3. Подготовка пилота  
  
- На этапе пилота выбираем вычислительно простой и устойчивый вариант, без «тяжелых» моделей.
- Что готовим до запуска:
  - стабильную версию модели и зафиксированные параметры;
  - выгрузку кандидатов/рекомендаций для тестовой группы;
  - резервный сценарий (популярные/новинки), если сервис модели недоступен;
  - логирование всех ключевых событий: показ, позиция, клик, корзина, покупка, версия модели, группа эксперимента.
- Ограничения по ресурсам в пилоте:
  - пересчет модели батчево (например, раз в сутки), без сложного онлайн-переобучения;
  - ограничиваем количество признаков и размер кандидатов, чтобы держать приемлемую задержку;
  - сначала запускаем на ограниченной доле трафика, затем расширяем при стабильных метриках.
- Как оцениваем вычислительную стоимость:
  - замеряем время обучения;
  - замеряем время подготовки рекомендаций;
  - замеряем задержку ответа в тестовой группе;
  - оцениваем объем хранения логов и артефактов моделей.
- План по шагам:
  - шаг 1: технический прогон на малом трафике и проверка логов;
  - шаг 2: запуск пилота на ограниченной аудитории;
  - шаг 3: промежуточный контроль метрик и качества данных;
  - шаг 4: финальный анализ и решение: масштабируем / дорабатываем / откатываем.

### 4. Внедрение `для production систем, если требуется`    

> Заполнение раздела 4 требуется не для всех дизайн документов. В некоторых случаях результатом итерации может быть расчет каких-то значений, далее используемых в бизнес-процессе для пилота.  
  
#### 4.1. Архитектура решения   
  
- Для прод-варианта используем простую двухшаговую схему: сначала отбор кандидатов, потом их финальная сортировка.
- Основные блоки:
  - **Сервис рекомендаций**: принимает запрос от продукта и возвращает `top-N` книг.
  - **Хранилище признаков/витрина**: хранит подготовленные признаки пользователей и книг.
  - **Сервис кандидатов**: быстро собирает короткий список подходящих книг.
  - **Сервис ранжирования**: пересчитывает и упорядочивает кандидатов по релевантности.
  - **Логи и мониторинг**: пишут события показа/клика/покупки и технические метрики.
  - **Резервный сценарий**: если модель недоступна, отдаем популярные/новые книги.
- Поток запроса:
  - продукт -> сервис рекомендаций -> отбор кандидатов -> сортировка -> бизнес-фильтры -> ответ продукту.
  
#### 4.2. Описание инфраструктуры и масштабируемости 
  
- **Какая инфраструктура выбрана и почему**
  - отдельные задачи по расписанию для дообучения и подготовки признаков;
  - отдельное хранилище для логов событий и артефактов моделей.
- **Плюсы выбора**
  - легко масштабировать сервис рекомендаций при росте трафика;
  - можно независимо обновлять модель и онлайн-сервис;
  - проще контролировать стабильность и откаты.
- **Минусы выбора**
  - появляется больше компонентов и нужна дисциплина по мониторингу;
  - выше операционная сложность, чем у «одного скрипта».
- **Почему этот вариант лучше альтернатив**
  - для MVP и первых релизов это самый практичный баланс между скоростью внедрения, стоимостью и надежностью;
  - более сложные варианты (полностью онлайн обучение, очень тяжелые модели) сейчас дадут больше затрат, чем пользы.
  
#### 4.3. Требования к работе системы  
  
- Целевые требования на MVP/первый прод:
  - доступность сервиса рекомендаций: не ниже `99.5%`;
  - задержка ответа: `p95 <= 200 мс` для запроса рекомендаций;
  - целевая нагрузка: `100-500` запросов в секунду.
- Если метрики ухудшаются:
  - включаем резервный сценарий;
  - снижаем размер кандидатов/признаков;
  
#### 4.4. Безопасность системы  
  
- Потенциальные уязвимости:
  - перегрузка сервиса частыми запросами;
  - накрутка пользовательских событий ботами;
  - ошибки конфигурации при выкладке новой версии модели.
- Что делаем:
  - ограничение частоты запросов и базовая защита от аномальной активности;
  - быстрый откат на последнюю стабильную версию.
  
#### 4.5. Безопасность данных   
  
- Работаем только с минимально необходимыми данными для рекомендаций.
- Основные меры:
  - не используем лишние персональные данные, которые не влияют на качество выдачи;
  - храним технические идентификаторы в обезличенном виде;
  - ограничиваем доступ к данным по ролям;
  - фиксируем сроки хранения логов и правила удаления устаревших данных.
- По комплаенсу:
  - соблюдаем локальные требования по персональным данным;
  - при работе с внешними данными проверяем лицензионные ограничения источников.
  
#### 4.6. Издержки  
  
- Основные статьи затрат в месяц:
  - вычисления для переобучения модели;
  - онлайн-инференс (обработка запросов рекомендаций);
  - хранение логов и артефактов моделей;
  - поддержка мониторинга и алертов.
- На этапе MVP ожидаем умеренные расходы, потому что:
  - обучение идет по расписанию, а не в реальном времени;
  - модель без избыточной сложности;
  - часть трафика может обслуживаться резервным сценарием.
- Подход к контролю затрат:
  - ежемесячно сверяем стоимость с приростом ключевых бизнес-метрик;
  - если эффект ниже ожиданий, упрощаем пайплайн или уменьшаем частоту пересчета.
  
#### 4.7. Точки интеграции  
  
- Основные точки интеграции:
  - фронт/бэкенд продукта -> сервис рекомендаций (получить `top-N`);
  - сервис рекомендаций -> хранилище признаков (получить признаки пользователя и книги);
  - сервис рекомендаций -> каталог/склад (проверить доступность книги);
  - сервис рекомендаций -> система логирования (события показа и клика);
  - сервис рекомендаций -> A/B система (получить группу пользователя).
- Формат взаимодействия:
  - синхронный запрос для выдачи рекомендаций;
  - асинхронная отправка событий в логи.
  
#### 4.8. Риски  
  
- Ключевые риски:
  - деградация качества после релиза из-за изменения поведения пользователей;
  - перекос в сторону популярных книг и падение разнообразия;
  - плохое качество логов и невозможность честно оценить эффект A/B;
  - рост задержки при увеличении трафика;
  - технический долг из-за быстрых MVP-решений.
- План снижения рисков:
  - постоянный мониторинг бизнес- и технических метрик;
  - регулярная перепроверка качества на warm/cold сегментах;
  - автоматические алерты на пропуски логов и скачки задержки;
  - регламент быстрого отката и обновления модели по расписанию.
